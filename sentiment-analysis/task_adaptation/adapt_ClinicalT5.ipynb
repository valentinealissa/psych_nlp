{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97431142-e663-4932-b8e5-d8336036a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, \\\n",
    "    TrainingArguments, T5ForConditionalGeneration\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "from pathlib import Path\n",
    "from pynvml import *\n",
    "import torch\n",
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import evaluate\n",
    "import math\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used // 1024 ** 2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daded22f-13be-4502-8425-37b79f327d86",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# parser = argparse.ArgumentParser(description='Task-adaptive model')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# parser.add_argument('--model', help='Path to BERT-like model')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# parser.add_argument('--model_name',\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#                     help='Name of the BERT-like model. Default = \"\" which corresponds to ClinicalBERT',\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#                     default='')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# config = parser.parse_args(sys.argv[1:])\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mluqh/ClinicalT5-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\" #minerva: download model from hugging face and put in folder, update to path\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# set seed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/modeling_utils.py:2755\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m from_flax:\n\u001b[1;32m   2754\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2755\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_flax_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_flax_checkpoint_in_pytorch_model\n\u001b[1;32m   2757\u001b[0m         model \u001b[38;5;241m=\u001b[39m load_flax_checkpoint_in_pytorch_model(model, resolved_archive_file)\n\u001b[1;32m   2758\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/modeling_flax_pytorch_utils.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnpicklingError\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/__init__.py:35\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Confusingly there are two things named \"config\": the module and the class.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# We want the exported object to be the class, so we first import the module\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# to make sure a later import doesn't overwrite the class.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m _config_module\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _config_module\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/config.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# TODO(phawkins): fix users of this alias and delete this file.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/config.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Callable, Hashable, NamedTuple, Iterator, Optional\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jax_jit\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transfer_guard_lib\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/lib/__init__.py:84\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Before importing any C compiled modules from jaxlib, first import the CPU\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# feature guard module to verify that jaxlib was compiled in a way that only\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# uses instructions that are present on this machine.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpu_feature_guard\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcpu_feature_guard\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mcpu_feature_guard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_cpu_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# TODO(phawkins): remove after minimium jaxlib version is 0.4.9 or newer.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='Task-adaptive model')\n",
    "# parser.add_argument('--model', help='Path to BERT-like model')\n",
    "# parser.add_argument('--model_name',\n",
    "#                     help='Name of the BERT-like model. Default = \"\" which corresponds to ClinicalBERT',\n",
    "#                     default='')\n",
    "# config = parser.parse_args(sys.argv[1:])\n",
    "\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained(\"luqh/ClinicalT5-base\", from_flax=True)\n",
    "\n",
    "#MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\" #minerva: download model from hugging face and put in folder, update to path\n",
    "\n",
    "# set seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e477406-ee0f-4c41-9cbf-70400a7d5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task Dataset\n",
    "task_dt_path = 'task_adapt_sentences.csv'\n",
    "sentences = pd.read_csv(task_dt_path, header=None)\n",
    "sentences = sentences[0].str.lower().str.split('.').values.tolist()\n",
    "sentences = list(itertools.chain.from_iterable(sentences))\n",
    "\n",
    "train, dev = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "# dev, test = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "task_dt = {'train': {},\n",
    "           'dev': {}}\n",
    "           #'test': {}}\n",
    "\n",
    "for s in train:\n",
    "    task_dt['train'].setdefault('text', list()).append(s)\n",
    "\n",
    "for s in dev:\n",
    "    task_dt['dev'].setdefault('text', list()).append(s)\n",
    "\n",
    "task_dt = DatasetDict({k: Dataset.from_dict(task_dt[k]) for k in task_dt.keys() if k != 'test'})\n",
    "task_dt.flatten()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"luqh/ClinicalT5-base\")\n",
    "tokenizer.add_tokens(['[DATE]', '[TIME]'], special_tokens=True)\n",
    "tkn_dt = task_dt.map(tokenize_function, batched=True, num_proc=4)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6c7e2-c178-4a48-9633-0961e4d71ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\n",
    "        MODEL,\n",
    "        from_tf=False).to('cuda')\n",
    "    print_gpu_utilization()\n",
    "else:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\n",
    "        MODEL,\n",
    "        from_tf=False)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "batch_size = 8 # [4, 8, 16]\n",
    "logging_steps = len(task_dt[\"train\"]) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"runs/ta_pretraining\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4, # [5e-6, 1e-5]\n",
    "    num_train_epochs=5, \n",
    "    weight_decay=0.01, # [1e-6, 1e-4, 1e-8]\n",
    "    warmup_ratio=0.01, #[1e-6, 1e-4, 1e-8]\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    logging_steps=logging_steps,\n",
    "    adam_epsilon=1e-6, # try to add AdamW optimizer post changing the batch size\n",
    "    seed=42\n",
    ") # seed = 42, data_seed = 42\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tkn_dt['train'],\n",
    "    eval_dataset=tkn_dt['dev'],\n",
    "    data_collator=data_collator   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3e211-fd87-45ba-8784-af1abd5e3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "print(\"\\n\\n\")\n",
    "result = trainer.train() #resume_from_checkpoint=True)\n",
    "print(result)\n",
    "print(\"\\n\\n\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
