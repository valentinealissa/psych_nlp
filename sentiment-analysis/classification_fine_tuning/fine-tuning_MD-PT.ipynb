{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcafa5-9711-4695-bdec-46e61e40845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, \\\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from pynvml import *\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from metrics import TaskMetrics\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used // 1024 ** 2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     scmetrics.add_batch(predictions=predictions, references=labels)\n",
    "#     return scmetrics.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aee683-4de0-4b69-845f-f3e7dfe3e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Sentence classification task')\n",
    "# parser.add_argument('--model', help='Path to pt model and tokenizer')\n",
    "# config = parser.parse_args(sys.argv[1:])\n",
    "# task = 'sentiment'\n",
    "#MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\" #minerva: download model from hugging face and put in folder, update to path\n",
    "MODEL = 'UFNLP/gatortron-base'\n",
    "\n",
    "\n",
    "# set seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create task Dataset from annotated samples\n",
    "sentences = pd.read_csv('13_words_39_sentences.csv', header=0)\n",
    "sentences = sentences[['language', \"MD_label_numeric\", \"PT_label_numeric\"]]\n",
    "sentences['MD_label_numeric'] = sentences['MD_label_numeric'].astype(float)\n",
    "sentences['PT_label_numeric'] = sentences['PT_label_numeric'].astype(float)\n",
    "\n",
    "dataset = Dataset.from_pandas(sentences).rename_columns({'language': 'sentence'})\n",
    "label_dt = dataset.train_test_split(0.5)\n",
    "\n",
    "cols = label_dt[\"train\"].column_names\n",
    "label_dt = label_dt.map(lambda x : {\"label\": [x[c] for c in cols if c != \"sentence\"]})\n",
    "label_dt = label_dt.remove_columns(['MD_label_numeric', 'PT_label_numeric'])\n",
    "\n",
    "# print(type(label_dt['train'][0]['label'][0]))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tkn_dt = label_dt.map(tokenize_function, batched=True, num_proc=4) # batched tokenizing activated\n",
    "# tkn_dt = tkn_dt.remove_columns(['']) # at some point we might need to delete sentiment column or else get an error\n",
    "\n",
    "# data loader = allows us to use a chunk of the data at a time while training (or else computer crashes)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # read the sentences and pad them to create equal length vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb7c6849-893b-43ba-95bb-666256fa4b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at runs/ta_pretraining/checkpoint-435 were not used when initializing MegatronBertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing MegatronBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MegatronBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MegatronBertForSequenceClassification were not initialized from the model checkpoint at runs/ta_pretraining/checkpoint-435 and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"runs/ta_pretraining/checkpoint-435\",\n",
    "                                                           num_labels=2,\n",
    "                                                           problem_type=\"multi_label_classification\",\n",
    "                                                           from_tf=False)\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda') # put the model on the gpu once, and then add a batch everytime when doing a training or evaluation loop\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "327a7b9e-ba84-4e00-ac6a-46e1b3ad592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'batch_size': 2, 'epochs': 1, 'learning_rate': 5e-06, 'warmup_ratio': 0, 'weight_decay': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1],\nInput references: [[1. 0.]\n [2. 2.]\n [0. 0.]\n [0. 0.]\n [2. 0.]\n [1. 1.]\n [0. 1.]\n [0. 0.]\n [0. 1.]\n [2. 2.]\n [0. 0.]\n [0. 2.]\n [1. 1.]\n [0. 0.]\n [0. 1.]\n [1. 1.]\n [0. 1.]\n [1. 1.]\n [0. 0.]\n [0. 2.]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 51\u001b[0m\n\u001b[1;32m     42\u001b[0m scmetrics \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     45\u001b[0m                   args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     46\u001b[0m                   callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)], \u001b[38;5;66;03m# if loss isnt decreasing for 2 epochs then it stops training\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m                   compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     50\u001b[0m                   data_collator\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[0;32m---> 51\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m results_eval \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     54\u001b[0m v \u001b[38;5;241m=\u001b[39m [comb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], comb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m], comb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], comb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m], comb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     55\u001b[0m results\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], results_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m], results_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_precision\u001b[39m\u001b[38;5;124m'\u001b[39m], results_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_recall\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:2034\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2034\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2037\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:2300\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2298\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2300\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:3029\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3026\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3028\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3029\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3032\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3033\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3039\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3314\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3315\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3316\u001b[0m         )\n\u001b[1;32m   3317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3318\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3320\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[26], line 37\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     35\u001b[0m logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m     36\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mscmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scmetrics\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/evaluate/module.py:512\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m     )\n\u001b[0;32m--> 512\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1],\nInput references: [[1. 0.]\n [2. 2.]\n [0. 0.]\n [0. 0.]\n [2. 0.]\n [1. 1.]\n [0. 1.]\n [0. 0.]\n [0. 1.]\n [2. 2.]\n [0. 0.]\n [0. 2.]\n [1. 1.]\n [0. 0.]\n [0. 1.]\n [1. 1.]\n [0. 1.]\n [1. 1.]\n [0. 0.]\n [0. 2.]]"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (for best configuration selection)\n",
    "# 1st evaluate the hyper parameters once at a time, and select each hyper parameter that gives you the best score on results aka F1 vs recall\n",
    "params = {\n",
    "    'batch_size': [2], # [2, 4, 8],\n",
    "    'epochs': [1], # [1, 2, 5],\n",
    "    'learning_rate': [5e-6], # [5e-6, 1e-5, 2e-5, 5e-5, 1e-4],\n",
    "    'weight_decay': [0], # [0, 0.01, 0.1], # how much the weight change is shrinking\n",
    "    'warmup_ratio': [0], # [0, 0.01, 0.1] # ratio of examples it takes to get ready for the learning rate\n",
    "} \n",
    "# best: \n",
    "# with task adapt:\n",
    "# w/o task adapt:\n",
    "\n",
    "metrics_file = f'classification_metrics_MD-PT.csv'\n",
    "if os.path.isfile(metrics_file):\n",
    "    f = open(metrics_file, 'a')\n",
    "else:\n",
    "    f = open(metrics_file, 'w')\n",
    "    f.write('batch_size,epochs,learning_rate,weight_decay,warmup_ratio,loss,f1,precision,recall\\n')\n",
    "\n",
    "best_model = []\n",
    "best_f1 = 0.0\n",
    "tmp_trainer, tmp_comb = None, None\n",
    "for comb in list(ParameterGrid(params)):\n",
    "    print(f\"Parameters: {comb}\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'runs/MD-PT',\n",
    "        evaluation_strategy='epoch', # every epoch the model is evaluated and checkpoint is made saving the weights\n",
    "        eval_steps=1, # check, probably each step is by epoch\n",
    "        logging_strategy='epoch',\n",
    "        weight_decay=comb['weight_decay'],\n",
    "        warmup_ratio=comb['warmup_ratio'],\n",
    "        num_train_epochs=comb['epochs'],\n",
    "        learning_rate=comb['learning_rate'],\n",
    "        per_device_train_batch_size=comb['batch_size'],\n",
    "        per_device_eval_batch_size=comb['batch_size'],\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_f1',\n",
    "        seed=42)\n",
    "    \n",
    "    scmetrics = evaluate.load(\"scmetrics\")\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      callbacks=[EarlyStoppingCallback(early_stopping_patience=10)], # if loss isnt decreasing for 2 epochs then it stops training\n",
    "                      train_dataset=tkn_dt['train'],\n",
    "                      eval_dataset=tkn_dt['test'],\n",
    "                      compute_metrics=compute_metrics,\n",
    "                      data_collator=data_collator)\n",
    "    results = trainer.train()\n",
    "    results_eval = trainer.evaluate()\n",
    "\n",
    "    # update to include: \n",
    "    v = [comb['batch_size'], comb['epochs'], comb['learning_rate'], comb['weight_decay'], comb['warmup_ratio'],\n",
    "    results.metrics['train_loss'], results_eval['eval_f1'], results_eval['eval_precision'], results_eval['eval_recall']]\n",
    "    f.write(','.join([str(el) for el in v]) + '\\n')\n",
    "\n",
    "    if results_eval['eval_f1'] > best_f1:\n",
    "        best_f1 = results_eval['eval_f1']\n",
    "        tmp_trainer = trainer\n",
    "        tmp_comb = comb\n",
    "    print('-' * 100)\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Error analysis step\n",
    "labels_to_sen = {0: 'neutral', 1: 'negative', 2: 'positive'}\n",
    "if tmp_trainer is not None:\n",
    "    best_trainer = tmp_trainer\n",
    "    best_comb = tmp_comb\n",
    "    print(f'Best parameters configuration: {best_comb}')\n",
    "    dev_pred = best_trainer.predict(tkn_dt['test'])\n",
    "    pred = np.argmax(dev_pred.predictions, axis=-1)\n",
    "    pred_score = np.max(torch.nn.functional.softmax(torch.tensor(dev_pred.predictions), dim=-1).numpy(), axis=-1)\n",
    "    i = 0\n",
    "    errors = {'FP': [], 'FN': []}\n",
    "    for pred_lab, true_lab in zip(pred, dev_pred.label_ids):\n",
    "        if pred_lab != true_lab:\n",
    "            if pred_lab > 1:\n",
    "                errors['FP'].append((\n",
    "                    tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tkn_dt['test']['input_ids'][i])),\n",
    "                    pred_score[i], labels_to_sen[pred_lab], labels_to_sen[true_lab]))\n",
    "            else:\n",
    "                errors['FN'].append((tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.convert_ids_to_tokens(tkn_dt['test']['input_ids'][i])), pred_score[i],\n",
    "                                     labels_to_sen[pred_lab], labels_to_sen[true_lab]))\n",
    "        i += 1\n",
    "    with open(f'error_analysis_MD-PT.tsv',\n",
    "              'w') as f:\n",
    "        f.write('sentence\\tpredicted_label\\ttrue_label\\tprobability\\n')\n",
    "        for k, vect in errors.items():\n",
    "            if k == 'FP':\n",
    "                for sen in vect:\n",
    "                    f.write(sen[0] + '\\t' + f'PRED_{sen[2].upper()}' + '\\t' + f'TRUE_{sen[3].upper()}' + '\\t' + str(\n",
    "                        sen[1]) + '\\n')\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                for sen in vect:\n",
    "                    f.write(sen[0] + '\\t' + f'PRED_{sen[2].upper()}' + '\\t' + f'TRUE_{sen[3].upper()}' + '\\t' + str(\n",
    "                        sen[1]) + '\\n')\n",
    "    test_pred = best_trainer.predict(tkn_dt['test'])\n",
    "    print(test_pred.metrics)\n",
    "\n",
    "    model_dir = f'runs/MD-PT'\n",
    "    for d in os.listdir(model_dir):\n",
    "        # This removes the checkpoints (comment it if you want to keep them)\n",
    "        if 'checkpoint' in d:\n",
    "            shutil.rmtree(os.path.join(model_dir, d))\n",
    "    best_trainer.save_model(\n",
    "        output_dir=f'best_model/MD-PT')\n",
    "else:\n",
    "    print(\"Precision is 0.0 change something in your model's configuration and retry.\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de640a-ab7a-4b09-a452-26b0b28589a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
