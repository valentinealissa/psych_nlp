{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d51ca5-7d92-433b-8520-bf462db8db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 18,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"She states that pt has been compliant with meds\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from openprompt.data_utils import InputExample\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_labels(sentiment):\n",
    "    labels = []\n",
    "    for s in sentiment:\n",
    "        if s == 'neutral':\n",
    "            labels += [0]\n",
    "        elif s == 'negative':\n",
    "            labels += [1]\n",
    "        else:\n",
    "            labels += [2]\n",
    "    return labels\n",
    "\n",
    "seed = 40\n",
    "# set seed\n",
    "\n",
    "# Create task Dataset from annotated samples\n",
    "sentences = pd.read_csv('../data/sentences_MD-labels.csv', header=0)\n",
    "sentences = sentences[['idx','language', \"MD_label\"]]\n",
    "\n",
    "dataset = Dataset.from_pandas(sentences).rename_columns({'language': 'sentence', \"MD_label\": 'sentiment'})\n",
    "dataset = dataset.add_column('label', create_labels(dataset['sentiment']))\n",
    "train_test = dataset.train_test_split(0.35, seed = seed)\n",
    "dev_test = train_test['test'].train_test_split(0.5, seed = seed)\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': dev_test['train'],\n",
    "    'test': dev_test['test']})\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a = data['sentence'], label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e1f9a6-5498-49d4-9c05-5973853ab760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type megatron-bert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ../runs/ta_pretraining/checkpoint-435 were not used when initializing BertForMaskedLM: ['bert.encoder.layer.16.attention.ln.weight', 'bert.encoder.layer.12.ln.bias', 'bert.encoder.layer.18.attention.ln.bias', 'bert.encoder.layer.14.attention.ln.weight', 'bert.encoder.layer.6.ln.weight', 'bert.encoder.layer.21.attention.ln.weight', 'bert.encoder.layer.12.ln.weight', 'bert.encoder.layer.3.attention.ln.weight', 'bert.encoder.layer.9.attention.ln.weight', 'bert.encoder.layer.2.ln.weight', 'bert.encoder.layer.1.ln.weight', 'bert.encoder.layer.9.ln.weight', 'bert.encoder.layer.20.attention.ln.weight', 'bert.encoder.layer.0.attention.ln.bias', 'bert.encoder.layer.7.ln.bias', 'bert.encoder.layer.20.ln.weight', 'bert.encoder.layer.5.attention.ln.bias', 'bert.encoder.layer.9.attention.ln.bias', 'bert.encoder.layer.4.ln.weight', 'bert.encoder.layer.13.ln.bias', 'bert.encoder.layer.19.ln.weight', 'bert.encoder.layer.15.ln.bias', 'bert.encoder.layer.22.ln.bias', 'bert.encoder.layer.8.attention.ln.weight', 'bert.encoder.layer.23.ln.bias', 'bert.encoder.layer.6.attention.ln.weight', 'bert.encoder.layer.3.attention.ln.bias', 'bert.encoder.layer.2.attention.ln.weight', 'bert.encoder.layer.10.ln.weight', 'bert.encoder.layer.2.attention.ln.bias', 'bert.encoder.layer.17.ln.bias', 'bert.encoder.layer.23.attention.ln.bias', 'bert.encoder.layer.16.ln.bias', 'bert.encoder.layer.19.attention.ln.bias', 'bert.encoder.layer.5.ln.bias', 'bert.encoder.layer.11.attention.ln.weight', 'bert.encoder.layer.15.attention.ln.weight', 'bert.encoder.layer.7.attention.ln.bias', 'bert.encoder.layer.11.attention.ln.bias', 'bert.encoder.layer.12.attention.ln.weight', 'bert.encoder.layer.8.ln.weight', 'bert.encoder.layer.23.ln.weight', 'bert.encoder.layer.17.ln.weight', 'bert.encoder.layer.4.attention.ln.bias', 'bert.encoder.layer.13.attention.ln.weight', 'bert.encoder.layer.8.attention.ln.bias', 'bert.encoder.layer.21.ln.weight', 'bert.encoder.layer.11.ln.bias', 'bert.encoder.layer.8.ln.bias', 'bert.encoder.ln.weight', 'bert.encoder.layer.20.attention.ln.bias', 'bert.encoder.layer.21.ln.bias', 'bert.encoder.layer.16.ln.weight', 'bert.encoder.layer.1.ln.bias', 'bert.encoder.layer.18.ln.weight', 'bert.encoder.layer.12.attention.ln.bias', 'bert.encoder.layer.13.attention.ln.bias', 'bert.encoder.layer.2.ln.bias', 'bert.encoder.layer.3.ln.bias', 'bert.encoder.layer.19.attention.ln.weight', 'bert.encoder.layer.9.ln.bias', 'bert.encoder.layer.15.ln.weight', 'bert.encoder.layer.10.ln.bias', 'bert.encoder.layer.16.attention.ln.bias', 'bert.encoder.layer.15.attention.ln.bias', 'bert.encoder.layer.14.ln.weight', 'bert.encoder.layer.1.attention.ln.bias', 'bert.encoder.layer.20.ln.bias', 'bert.encoder.layer.4.ln.bias', 'bert.encoder.layer.17.attention.ln.weight', 'bert.encoder.layer.7.ln.weight', 'bert.encoder.layer.17.attention.ln.bias', 'bert.encoder.layer.19.ln.bias', 'bert.encoder.layer.6.ln.bias', 'bert.encoder.layer.5.attention.ln.weight', 'bert.encoder.layer.22.attention.ln.bias', 'bert.encoder.layer.3.ln.weight', 'bert.encoder.layer.22.attention.ln.weight', 'bert.encoder.layer.1.attention.ln.weight', 'bert.encoder.layer.23.attention.ln.weight', 'bert.encoder.ln.bias', 'bert.encoder.layer.14.attention.ln.bias', 'bert.encoder.layer.0.attention.ln.weight', 'bert.encoder.layer.6.attention.ln.bias', 'bert.encoder.layer.0.ln.bias', 'bert.encoder.layer.5.ln.weight', 'bert.encoder.layer.13.ln.weight', 'bert.encoder.layer.18.ln.bias', 'bert.encoder.layer.10.attention.ln.weight', 'bert.encoder.layer.21.attention.ln.bias', 'bert.encoder.layer.22.ln.weight', 'bert.encoder.layer.7.attention.ln.weight', 'bert.encoder.layer.14.ln.bias', 'bert.encoder.layer.0.ln.weight', 'bert.encoder.layer.4.attention.ln.weight', 'bert.encoder.layer.10.attention.ln.bias', 'bert.encoder.layer.18.attention.ln.weight', 'bert.encoder.layer.11.ln.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ../runs/ta_pretraining/checkpoint-435 and are newly initialized: ['bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'She states that pt has been compliant with meds', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 18, 'label': 0}]\n",
      "{'input_ids': [101, 627, 1692, 323, 724, 394, 636, 10604, 189, 4013, 653, 245, 103, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'loss_ids': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "['[CLS]', 'she', 'states', 'that', 'pt', 'has', 'been', 'compliant', 'with', 'meds', 'it', 'was', '[MASK]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# You can load the plm related things provided by openprompt simply by calling:\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, set_seed\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from mlm import MLMTokenizerWrapper\n",
    "import torch\n",
    "\n",
    "set_seed(seed)\n",
    "MODEL = 'UFNLP/gatortron-base'\n",
    "model_path = \"../runs/ta_pretraining/checkpoint-435\"\n",
    "model_config = BertConfig.from_pretrained(model_path)\n",
    "plm = BertForMaskedLM.from_pretrained(model_path, config=model_config)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "WrapperClass = MLMTokenizerWrapper\n",
    "\n",
    "\n",
    "# Constructing Template\n",
    "# A template can be constructed from the yaml config, but it can also be constructed by directly passing arguments.\n",
    "\n",
    "template_text = '{\"placeholder\":\"text_a\"} It was {\"mask\"}'\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
    "\n",
    "# To better understand how does the template wrap the example, we visualize one instance.\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)\n",
    "\n",
    "# Now, the wrapped example is ready to be pass into the tokenizer, hence producing the input for language models.\n",
    "# You can use the tokenizer to tokenize the input by yourself, but we recommend using our wrapped tokenizer, which is a wrapped tokenizer tailed for InputExample.\n",
    "# The wrapper has been given if you use our `load_plm` function, otherwise, you should choose the suitable wrapper based on\n",
    "# the configuration in `openprompt.plms.__init__.py`.\n",
    "# Note that when t5 is used for classification, we only need to pass <pad> <extra_id_0> <eos> to decoder.\n",
    "# The loss is calcaluted at <extra_id_0>. Thus passing decoder_max_length=3 saves the space\n",
    "wrapped_tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "# or\n",
    "# from openprompt.plms import BERTTokenizerWrapper\n",
    "# wrapped_tokenizer= BERTTokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "\n",
    "# You can see what a tokenized example looks like by\n",
    "tokenized_example = wrapped_tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
    "print(tokenized_example)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))\n",
    "# print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04910fd7-8e2b-4544-9d27-8fd9619a53d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 0it [00:00, ?it/s]Fatal Python error: config_get_locale_encoding: failed to get the locale encoding: nl_langinfo(CODESET) failed\n",
      "Python runtime state: preinitialized\n",
      "\n",
      "tokenizing: 30it [00:00, 1182.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now it's time to convert the whole dataset into the input format!\n",
    "# Simply loop over the dataset to achieve it!\n",
    "\n",
    "model_inputs = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    model_inputs[split] = []\n",
    "    for sample in dataset[split]:\n",
    "        tokenized_example = wrapped_tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
    "        model_inputs[split].append(tokenized_example)\n",
    "\n",
    "# We provide a `PromptDataLoader` class to help you do all the above matters and wrap them into an `torch.DataLoader` style iterator.\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4022ebb5-c70d-434a-98d6-3f5476ad3cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 7550,     0],\n",
      "         [13163,     0],\n",
      "         [39429,     0],\n",
      "         [ 5221,     0]],\n",
      "\n",
      "        [[ 8837,     0],\n",
      "         [ 2639,  1823],\n",
      "         [49283,     0],\n",
      "         [47322,  1083]],\n",
      "\n",
      "        [[ 1975,     0],\n",
      "         [36986,  1823],\n",
      "         [ 2203,     0],\n",
      "         [ 3565,     0]]])\n",
      "tensor([[-2.5886, -2.8751, -3.5636],\n",
      "        [-3.1815, -2.5237, -2.4045]])\n"
     ]
    }
   ],
   "source": [
    "# Define the verbalizer\n",
    "# In classification, you need to define your verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# # for example the verbalizer contains multiple label words in each class\n",
    "# myverbalizer = ManualVerbalizer(tokenizer, num_classes=3,\n",
    "#                         label_words=[[\"neutral\"], [\"negative\"], [\"positive\"]])\n",
    "\n",
    "classes = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
    "    \"neutral\",\n",
    "    \"negative\",\n",
    "    \"positive\"\n",
    "]\n",
    "myverbalizer = ManualVerbalizer(\n",
    "    tokenizer = tokenizer,\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"neutral\": [\"fair\", \"okay\", \"unbiased\", \"unknown\"],\n",
    "        \"negative\": [\"bad\", \"awful\", \"terrible\", \"horrible\"],\n",
    "        \"positive\": [\"good\", \"wonderful\", \"great\", \"effective\"],\n",
    "    },   \n",
    ")\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
    "print(myverbalizer.process_logits(logits)) # see what the verbalizer do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d42991-f193-4394-9a14-6e4110f10d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valena17/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 3.030619263648987\n",
      "Epoch 1, average loss: 1.3008103966712952\n",
      "Epoch 2, average loss: 1.0234407186508179\n",
      "Epoch 3, average loss: 1.022293210029602\n",
      "Epoch 4, average loss: 1.0564102530479431\n",
      "Epoch 5, average loss: 1.0250256061553955\n",
      "Epoch 6, average loss: 0.621261477470398\n",
      "Epoch 7, average loss: 0.5856716930866241\n",
      "Epoch 8, average loss: 0.36010362207889557\n",
      "Epoch 9, average loss: 0.7198853194713593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 8it [00:00, 1500.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    }
   ],
   "source": [
    "# Although you can manually combine the plm, template, verbalizer together, we provide a pipeline\n",
    "# model which take the batched data from the PromptDataLoader and produce a class-wise logits\n",
    "\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = False\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=prompt_model.cuda()\n",
    "\n",
    "# Now the training is standard\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-6)\n",
    "\n",
    "for epoch in range(10):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "# Evaluate\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2c72e-4710-435d-80b9-b9bd08f4619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4, batch=4, epoch=4\n",
    "Epoch 0, average loss: 1.6096126437187195\n",
    "Epoch 1, average loss: 2.4815704226493835\n",
    "Epoch 2, average loss: 4.7398681640625\n",
    "Epoch 3, average loss: 1.7757583856582642\n",
    "acc = 0.25\n",
    "\n",
    "lr=1e-6, batch=4, epoch=4\n",
    "Epoch 0, average loss: 5.752278208732605\n",
    "Epoch 1, average loss: 2.5698354244232178\n",
    "Epoch 2, average loss: 2.4523871019482613\n",
    "Epoch 3, average loss: 0.7844662666320801\n",
    "acc = 0.25\n",
    "\n",
    "-------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
