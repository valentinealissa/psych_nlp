{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4b09d2-7792-4a2c-bf74-7a590032580e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valena17/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, \\\n",
    "    TrainingArguments, T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt import PromptForClassification, PromptDataLoader\n",
    "from collections import namedtuple\n",
    "from seq2seq import T5LMTokenizerWrapper, T5TokenizerWrapper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad3f738-d401-47f4-b5ea-7dac62ecf017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\n",
    "\n",
    "_MODEL_CLASSES = {\n",
    "    't5':ModelClass(**{\n",
    "        'config': T5Config,\n",
    "        'tokenizer': T5Tokenizer,\n",
    "        'model': T5ForConditionalGeneration,\n",
    "        'wrapper': T5TokenizerWrapper\n",
    "    }),\n",
    "}\n",
    "\n",
    "def get_model_class(plm_type: str):\n",
    "    return _MODEL_CLASSES[plm_type]\n",
    "\n",
    "def load_plm(model_name, model_path, specials_to_add = None):\n",
    "    r\"\"\"A plm loader using a global config.\n",
    "    It will load the model, tokenizer, and config simulatenously.\n",
    "\n",
    "    Args:\n",
    "        config (:obj:`CfgNode`): The global config from the CfgNode.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`PreTrainedModel`: The pretrained model.\n",
    "        :obj:`tokenizer`: The pretrained tokenizer.\n",
    "        :obj:`model_config`: The config of the pretrained model.\n",
    "        :obj:`wrapper`: The wrapper class of this plm.\n",
    "    \"\"\"\n",
    "    model_class = get_model_class(plm_type = model_name)\n",
    "    model_config = model_class.config.from_pretrained(model_path, from_flax=True)\n",
    "    # you can change huggingface model_config here\n",
    "    # if 't5'  in model_name: # remove dropout according to PPT~\\ref{}\n",
    "    #     model_config.dropout_rate = 0.0\n",
    "    if 'gpt' in model_name: # add pad token for gpt\n",
    "        specials_to_add = [\"<pad>\"]\n",
    "        # model_config.attn_pdrop = 0.0\n",
    "        # model_config.resid_pdrop = 0.0\n",
    "        # model_config.embd_pdrop = 0.0\n",
    "    model = model_class.model.from_pretrained(model_path, config=model_config, from_flax=True)\n",
    "    tokenizer = model_class.tokenizer.from_pretrained(model_path)\n",
    "    wrapper = model_class.wrapper\n",
    "\n",
    "\n",
    "    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=specials_to_add)\n",
    "\n",
    "    if 'opt' in model_name:\n",
    "        tokenizer.add_bos_token=False\n",
    "    return model, tokenizer, model_config, wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0c20d3-254e-4e08-8c1d-20d81e6395bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MODEL = T5ForConditionalGeneration.from_pretrained(\"luqh/ClinicalT5-base\", from_flax=True)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plm, tokenizer, model_config, WrapperClass \u001b[38;5;241m=\u001b[39m \u001b[43mload_plm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mluqh/ClinicalT5-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mload_plm\u001b[0;34m(model_name, model_path, specials_to_add)\u001b[0m\n\u001b[1;32m     34\u001b[0m     specials_to_add \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# model_config.attn_pdrop = 0.0\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# model_config.resid_pdrop = 0.0\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# model_config.embd_pdrop = 0.0\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m     40\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mwrapper\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/modeling_utils.py:2755\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m from_flax:\n\u001b[1;32m   2754\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2755\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_flax_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_flax_checkpoint_in_pytorch_model\n\u001b[1;32m   2757\u001b[0m         model \u001b[38;5;241m=\u001b[39m load_flax_checkpoint_in_pytorch_model(model, resolved_archive_file)\n\u001b[1;32m   2758\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/modeling_flax_pytorch_utils.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnpicklingError\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/__init__.py:35\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Confusingly there are two things named \"config\": the module and the class.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# We want the exported object to be the class, so we first import the module\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# to make sure a later import doesn't overwrite the class.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m _config_module\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _config_module\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/config.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The JAX Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# TODO(phawkins): fix users of this alias and delete this file.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/config.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Callable, Hashable, NamedTuple, Iterator, Optional\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jax_jit\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transfer_guard_lib\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/lib/__init__.py:84\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Before importing any C compiled modules from jaxlib, first import the CPU\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# feature guard module to verify that jaxlib was compiled in a way that only\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# uses instructions that are present on this machine.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpu_feature_guard\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcpu_feature_guard\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mcpu_feature_guard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_cpu_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# TODO(phawkins): remove after minimium jaxlib version is 0.4.9 or newer.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "source": [
    "# MODEL = T5ForConditionalGeneration.from_pretrained(\"luqh/ClinicalT5-base\", from_flax=True)\n",
    "\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"luqh/ClinicalT5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741fade-effb-4eba-8b49-4e1dfe4d9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} It was {\"mask\"}',\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = classes,\n",
    "    label_words = {\n",
    "        \"negative\": [\"bad\"],\n",
    "        \"positive\": [\"good\", \"wonderful\", \"great\"],\n",
    "    },\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer,\n",
    ")\n",
    "\n",
    "\n",
    "data_loader = PromptDataLoader(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    template = promptTemplate,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# making zero-shot inference using pretrained MLM with prompt\n",
    "promptModel.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        logits = promptModel(batch)\n",
    "        preds = torch.argmax(logits, dim = -1)\n",
    "        print(classes[preds])\n",
    "# predictions would be 1, 0 for classes 'positive', 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8cc05-7e32-47e0-820c-0eec268b4921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
