{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc05de93-7b10-468e-9fca-b90a979626ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valena17/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "You are using a model of type megatron-bert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ../runs/ta_pretraining/checkpoint-435 were not used when initializing BertForMaskedLM: ['bert.encoder.layer.2.ln.weight', 'bert.encoder.layer.16.ln.bias', 'bert.encoder.layer.11.ln.bias', 'bert.encoder.layer.9.attention.ln.bias', 'bert.encoder.layer.6.ln.bias', 'bert.encoder.layer.1.attention.ln.bias', 'bert.encoder.layer.21.ln.bias', 'bert.encoder.layer.5.ln.bias', 'bert.encoder.layer.14.ln.bias', 'bert.encoder.ln.bias', 'bert.encoder.layer.17.attention.ln.weight', 'bert.encoder.layer.20.ln.bias', 'bert.encoder.layer.15.attention.ln.bias', 'bert.encoder.layer.14.ln.weight', 'bert.encoder.layer.2.attention.ln.bias', 'bert.encoder.layer.14.attention.ln.bias', 'bert.encoder.layer.21.attention.ln.bias', 'bert.encoder.layer.5.ln.weight', 'bert.encoder.layer.15.attention.ln.weight', 'bert.encoder.layer.8.ln.weight', 'bert.encoder.layer.1.attention.ln.weight', 'bert.encoder.layer.7.ln.bias', 'bert.encoder.layer.2.ln.bias', 'bert.encoder.layer.17.attention.ln.bias', 'bert.encoder.layer.4.attention.ln.bias', 'bert.encoder.layer.0.attention.ln.bias', 'bert.encoder.layer.1.ln.bias', 'bert.encoder.layer.4.ln.weight', 'bert.encoder.layer.19.attention.ln.weight', 'bert.encoder.layer.3.ln.bias', 'bert.encoder.layer.3.attention.ln.weight', 'bert.encoder.layer.4.ln.bias', 'bert.encoder.layer.8.attention.ln.bias', 'bert.encoder.layer.10.attention.ln.bias', 'bert.encoder.layer.20.attention.ln.weight', 'bert.encoder.layer.22.attention.ln.bias', 'bert.encoder.layer.15.ln.weight', 'bert.encoder.layer.10.ln.bias', 'bert.encoder.layer.18.ln.weight', 'bert.encoder.layer.16.ln.weight', 'bert.encoder.layer.23.ln.weight', 'bert.encoder.layer.10.attention.ln.weight', 'bert.encoder.layer.11.attention.ln.weight', 'bert.encoder.layer.11.ln.weight', 'bert.encoder.layer.16.attention.ln.weight', 'bert.encoder.layer.23.attention.ln.weight', 'bert.encoder.layer.4.attention.ln.weight', 'bert.encoder.layer.3.attention.ln.bias', 'bert.encoder.layer.13.attention.ln.weight', 'bert.encoder.layer.3.ln.weight', 'bert.encoder.layer.18.attention.ln.bias', 'bert.encoder.layer.8.ln.bias', 'bert.encoder.layer.13.ln.weight', 'bert.encoder.layer.21.attention.ln.weight', 'bert.encoder.layer.16.attention.ln.bias', 'bert.encoder.layer.0.attention.ln.weight', 'bert.encoder.layer.20.ln.weight', 'bert.encoder.layer.2.attention.ln.weight', 'bert.encoder.layer.18.ln.bias', 'bert.encoder.layer.12.attention.ln.weight', 'bert.encoder.layer.17.ln.bias', 'bert.encoder.layer.23.ln.bias', 'bert.encoder.layer.22.ln.weight', 'bert.encoder.layer.21.ln.weight', 'bert.encoder.layer.9.attention.ln.weight', 'bert.encoder.layer.7.ln.weight', 'bert.encoder.layer.22.attention.ln.weight', 'bert.encoder.layer.1.ln.weight', 'bert.encoder.layer.6.ln.weight', 'bert.encoder.layer.13.ln.bias', 'bert.encoder.layer.0.ln.bias', 'bert.encoder.layer.7.attention.ln.bias', 'bert.encoder.layer.6.attention.ln.weight', 'bert.encoder.layer.7.attention.ln.weight', 'bert.encoder.layer.12.ln.bias', 'bert.encoder.layer.19.ln.weight', 'bert.encoder.layer.9.ln.weight', 'bert.encoder.layer.9.ln.bias', 'bert.encoder.layer.0.ln.weight', 'bert.encoder.layer.20.attention.ln.bias', 'bert.encoder.layer.10.ln.weight', 'bert.encoder.layer.19.ln.bias', 'bert.encoder.layer.8.attention.ln.weight', 'bert.encoder.layer.14.attention.ln.weight', 'bert.encoder.layer.5.attention.ln.bias', 'bert.encoder.layer.5.attention.ln.weight', 'bert.encoder.layer.17.ln.weight', 'bert.encoder.layer.22.ln.bias', 'bert.encoder.layer.12.attention.ln.bias', 'bert.encoder.layer.6.attention.ln.bias', 'bert.encoder.layer.18.attention.ln.weight', 'bert.encoder.layer.11.attention.ln.bias', 'bert.encoder.layer.23.attention.ln.bias', 'bert.encoder.layer.12.ln.weight', 'bert.encoder.layer.15.ln.bias', 'bert.encoder.ln.weight', 'bert.encoder.layer.13.attention.ln.bias', 'bert.encoder.layer.19.attention.ln.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ../runs/ta_pretraining/checkpoint-435 and are newly initialized: ['bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from openprompt.data_utils import PROCESSORS\n",
    "import torch\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import argparse\n",
    "import numpy as np\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.prompts import SoftTemplate\n",
    "from openprompt import PromptForClassification\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(\"\")\n",
    "# parser.add_argument(\"--shot\", type=int, default=-1)\n",
    "# parser.add_argument(\"--seed\", type=int, default=144)\n",
    "# parser.add_argument(\"--plm_eval_mode\", action=\"store_true\", help=\"whether to turn off the dropout in the freezed model. Set to true to turn off.\")\n",
    "# parser.add_argument(\"--tune_plm\", action=\"store_true\")\n",
    "# parser.add_argument(\"--model\", type=str, default='t5-lm', help=\"We test both t5 and t5-lm in this scripts, the corresponding tokenizerwrapper will be automatically loaded.\")\n",
    "# parser.add_argument(\"--model_name_or_path\", default='../../plm_cache/t5-large-lm-adapt/')\n",
    "# parser.add_argument(\"--project_root\", default=\"/mnt/sfs_turbo/hsd/OpenPrompt_official/OpenPrompt/\", help=\"The project root in the file system, i.e. the absolute path of OpenPrompt\")\n",
    "# parser.add_argument(\"--template_id\", type=int)\n",
    "# parser.add_argument(\"--verbalizer_id\", type=int)\n",
    "# parser.add_argument(\"--data_dir\", type=str, default=\"/mnt/sfs_turbo/huggingface_datasets/\") # sometimes, huggingface datasets can not be automatically downloaded due to network issue, please refer to 0_basic.py line 15 for solutions.\n",
    "# parser.add_argument(\"--dataset\",type=str)\n",
    "# parser.add_argument(\"--result_file\", type=str, default=\"../sfs_out/results.txt\")\n",
    "# parser.add_argument(\"--max_steps\", default=20000, type=int)\n",
    "# parser.add_argument(\"--prompt_lr\", type=float, default=0.3)\n",
    "# parser.add_argument(\"--warmup_step_prompt\", type=int, default=500)\n",
    "# parser.add_argument(\"--init_from_vocab\", action=\"store_false\")\n",
    "# parser.add_argument(\"--eval_every_steps\", type=int, default=500)\n",
    "# parser.add_argument(\"--soft_token_num\", type=int, default=20)\n",
    "# parser.add_argument(\"--optimizer\", type=str, default=\"Adafactor\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.result_file = os.path.join(args.project_root, args.result_file)\n",
    "\n",
    "# content_write = \"=\"*20+\"\\n\"\n",
    "# content_write += f\"dataset {args.dataset}\\t\"\n",
    "# content_write += f\"temp {args.template_id}\\t\"\n",
    "# content_write += f\"verb {args.verbalizer_id}\\t\"\n",
    "# content_write += f\"model {args.model}\\t\"\n",
    "# content_write += f\"seed {args.seed}\\t\"\n",
    "# content_write += f\"shot {args.shot}\\t\"\n",
    "# content_write += f\"plm_eval_mode {args.plm_eval_mode}\\t\"\n",
    "# content_write += f\"init_from_vocab {args.init_from_vocab}\\t\"\n",
    "# content_write += f\"eval_every_steps {args.eval_every_steps}\\t\"\n",
    "# content_write += f\"prompt_lr {args.prompt_lr}\\t\"\n",
    "# content_write += f\"optimizer {args.optimizer}\\t\"\n",
    "# content_write += f\"warmup_step_prompt {args.warmup_step_prompt}\\t\"\n",
    "# content_write += f\"soft_token_num {args.soft_token_num}\\t\"\n",
    "# content_write += \"\\n\"\n",
    "\n",
    "# print(content_write)\n",
    "\n",
    "import random\n",
    "this_run_unicode = str(random.randint(0, 1e10))\n",
    "\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# use lm-adapted version or t5-v1.1 checkpoint. Note that the original t5 checkpoint has been pretrained\n",
    "# on part of GLUE dataset, thus should not be used.\n",
    "# from openprompt.plms.seq2seq import T5TokenizerWrapper, T5LMTokenizerWrapper\n",
    "from mlm import MLMTokenizerWrapper\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, set_seed\n",
    "# from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "from openprompt.data_utils.data_sampler import FewShotSampler\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "MODEL = 'UFNLP/gatortron-base'\n",
    "model_path = \"../runs/ta_pretraining/checkpoint-435\"\n",
    "model_config = BertConfig.from_pretrained(model_path)\n",
    "plm = BertForMaskedLM.from_pretrained(model_path, config=model_config)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "WrapperClass = MLMTokenizerWrapper\n",
    "#plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57e166c-e0af-4e7b-8c50-fb25fcf2b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 30,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"His worsening psychotic symptom secondary to medication non compliance and substance use (utox (+) cocaine/cannabis), will admit for safety\",\n",
      "  \"text_b\": \"Input: Sister saw pt often while he was at XXX b/c she works the night shift and would visit him on her breaks; states he was always quite sweet, not agitated, but sometimes thought she was their  mother, or another sister. Prompt: What is the sentiment of the sentence? Output: positive.\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = {}\n",
    "\n",
    "# Below are multiple dataset examples, including few-shot ones.\n",
    "# if args.dataset == \"boolq\":\n",
    "#     Processor = PROCESSORS[\"super_glue.boolq\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/BoolQ\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480 # this should be specified according to the running GPU's capacity\n",
    "#     if args.tune_plm: # tune the entire plm will use more gpu-memories, thus we should use a smaller batch_size.\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True # if multiple gpus are available, one can use model_parallelize\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"multirc\":\n",
    "#     Processor = PROCESSORS[\"super_glue.multirc\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/MultiRC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"rte\":\n",
    "#     Processor = PROCESSORS[\"super_glue.rte\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/RTE\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 2\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"cb\":\n",
    "#     Processor = PROCESSORS[\"super_glue.cb\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/CB\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"wic\":\n",
    "#     Processor = PROCESSORS[\"super_glue.wic\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/WiC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"fewshot_boolq\":\n",
    "#     Processor = PROCESSORS[\"super_glue.boolq\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/BoolQ\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     sampler = FewShotSampler(num_examples_per_label=32)\n",
    "#     dataset['train']= sampler(dataset['train'], seed=args.seed)\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"fewshot_multirc\":\n",
    "#     Processor = PROCESSORS[\"super_glue.multirc\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/MultiRC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     sampler = FewShotSampler(num_examples_per_label=32)\n",
    "#     dataset['train']= sampler(dataset['train'], seed=args.seed)\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"fewshot_wic\":\n",
    "#     Processor = PROCESSORS[\"super_glue.wic\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/WiC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     sampler = FewShotSampler(num_examples_per_label=32)\n",
    "#     dataset['train']= sampler(dataset['train'], seed=args.seed)\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# else:\n",
    "#     raise NotImplementedError\n",
    "\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "from openprompt.data_utils import InputExample\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_labels(sentiment):\n",
    "    labels = []\n",
    "    for s in sentiment:\n",
    "        if s == 'neutral':\n",
    "            labels += [0]\n",
    "        elif s == 'negative':\n",
    "            labels += [1]\n",
    "        else:\n",
    "            labels += [2]\n",
    "    return labels\n",
    "\n",
    "# Create task Dataset from annotated samples\n",
    "sentences = pd.read_csv('../data/sentences_MD_wContext.csv', header=0)\n",
    "sentences = sentences[['idx','language', \"MD_label\", \"context\"]]\n",
    "\n",
    "dataset = Dataset.from_pandas(sentences).rename_columns({'language': 'sentence', \"MD_label\": 'sentiment'})\n",
    "dataset = dataset.add_column('label', create_labels(dataset['sentiment']))\n",
    "train_test = dataset.train_test_split(0.35, seed = seed)\n",
    "dev_test = train_test['test'].train_test_split(0.5, seed = seed)\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': dev_test['train'],\n",
    "    'test': dev_test['test']})\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a = data['sentence'], text_b = data['context'], \n",
    "                                     label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "\n",
    "\n",
    "# class_labels =Processor().get_labels() # what do i need to update this to? best bet below\n",
    "# class_labels = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
    "#     \"neutral\",\n",
    "#     \"negative\",\n",
    "#     \"positive\"\n",
    "# ]\n",
    "class_labels = [0, 1, 2]\n",
    "\n",
    "sampler = FewShotSampler(num_examples_per_label=32)\n",
    "dataset['train']= sampler(dataset['train'], seed=seed)\n",
    "max_seq_l = 480\n",
    "batchsize_t = 4\n",
    "batchsize_e = 4\n",
    "gradient_accumulation_steps = 8\n",
    "model_parallelize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621e1922-24ec-4386-91d2-c32ed786bab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'Context:', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': 'Input: Pt continues to report adherence with his medication; cannot offer explanation as to why his symptoms were worse yesterday. Prompt: What is the sentiment of the sentence? Output: neutral.', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '. Input:', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': 'he does not cooperate with treatment decisions when he is manic-- shows poor judgement such as not going to his XXX appointments or taking correct medication', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '. Prompt: What is the sentiment of the sentence?', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 36, 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 30it [00:00, 485.21it/s]\n",
      "tokenizing: 8it [00:00, 592.28it/s]\n",
      "tokenizing: 9it [00:00, 654.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncate rate: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now define the template and verbalizer.\n",
    "# Note that soft template can be combined with hard template, by loading the hard template from file.\n",
    "# For example, the template in soft_template.txt is {}\n",
    "# The choice_id 1 is the hard template\n",
    "mytemplate = SoftTemplate(model=plm, # what should these paramters be? taking value from example on open prompt\n",
    "                          tokenizer=tokenizer,\n",
    "                          text='Context:{\"placeholder\":\"text_b\"}. Input:{\"placeholder\":\"text_a\"}. Prompt: What is the sentiment of the sentence? {\"mask\"}')\n",
    "\n",
    "myverbalizer = ManualVerbalizer(tokenizer, \n",
    "                                classes = class_labels,\n",
    "                                label_words = {0 : \"neutral\",\n",
    "                                              1 : \"negative\",\n",
    "                                              2 : \"positive\"}) # (Union[List[str], List[List[str]], Dict[List[str]]], optional) â€“ The label words that are projected by the labels.\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)\n",
    "\n",
    "\n",
    "# use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm, \n",
    "                                       template=mytemplate, \n",
    "                                       verbalizer=myverbalizer, \n",
    "                                       freeze_plm=True, \n",
    "                                       plm_eval_mode=True) # unclear if True is stronger or weaker freezing)\n",
    "# if use_cuda:\n",
    "#     prompt_model=  prompt_model.cuda()\n",
    "\n",
    "# if model_parallelize:\n",
    "#     prompt_model.parallelize()\n",
    "\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "    batch_size=batchsize_t,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "    batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "# zero-shot test\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "    batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "print(\"truncate rate: {}\".format(test_dataloader.tokenizer_wrapper.truncate_rate), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830647d-7f13-4686-b59d-b6ca35e24756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:   0%|                                                                                             | 0/20000 [02:14<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin epoch 0\n",
      "Begin epoch 1\n",
      "Begin epoch 2\n"
     ]
    }
   ],
   "source": [
    "def evaluate(prompt_model, dataloader, desc):\n",
    "    prompt_model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup,get_constant_schedule_with_warmup  # use AdamW is a standard practice for transformer\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule  # use Adafactor is the default setting for T5 --> not sure what to use for others\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "tot_step = 20000 # max steps?\n",
    "\n",
    "\n",
    "# if args.tune_plm: # normally we freeze the model when using soft_template. However, we keep the option to tune plm\n",
    "#     no_decay = ['bias', 'LayerNorm.weight'] # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "#     optimizer_grouped_parameters1 = [\n",
    "#         {'params': [p for n, p in prompt_model.plm.named_parameters() if (not any(nd in n for nd in no_decay))], 'weight_decay': 0.01},\n",
    "#         {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#     ]\n",
    "#     optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
    "#     scheduler1 = get_linear_schedule_with_warmup(\n",
    "#         optimizer1,\n",
    "#         num_warmup_steps=500, num_training_steps=tot_step)\n",
    "# else:\n",
    "#     optimizer1 = None\n",
    "#     scheduler1 = None\n",
    "\n",
    "\n",
    "optimizer_grouped_parameters2 = [{'params': [p for name, p in prompt_model.template.named_parameters() if 'raw_embedding' not in name]}] # note that you have to remove the raw_embedding manually from the optimization\n",
    "# if args.optimizer.lower() == \"adafactor\":\n",
    "#     optimizer2 = Adafactor(optimizer_grouped_parameters2,\n",
    "#                             lr=args.prompt_lr,\n",
    "#                             relative_step=False,\n",
    "#                             scale_parameter=False,\n",
    "#                             warmup_init=False)  # when lr is 0.3, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
    "#     scheduler2 = get_constant_schedule_with_warmup(optimizer2, num_warmup_steps=args.warmup_step_prompt) # when num_warmup_steps is 0, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
    "# elif args.optimizer.lower() == \"adamw\":\n",
    "#     optimizer2 = AdamW(optimizer_grouped_parameters2, lr=args.prompt_lr) # usually lr = 0.5\n",
    "#     scheduler2 = get_linear_schedule_with_warmup(\n",
    "#                     optimizer2,\n",
    "#                     num_warmup_steps=args.warmup_step_prompt, num_training_steps=tot_step) # usually num_warmup_steps is 500\n",
    "\n",
    "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=0.5) # usually lr = 0.5\n",
    "scheduler2 = get_linear_schedule_with_warmup(optimizer2,\n",
    "                                             num_warmup_steps=500, # taken from open prompt example\n",
    "                                             num_training_steps=tot_step) # usually num_warmup_steps is 500\n",
    "\n",
    "\n",
    "tot_loss = 0\n",
    "log_loss = 0\n",
    "best_val_acc = 0\n",
    "glb_step = 0\n",
    "actual_step = 0\n",
    "leave_training = False\n",
    "\n",
    "acc_traces = []\n",
    "tot_train_time = 0\n",
    "pbar_update_freq = 10\n",
    "prompt_model.train()\n",
    "\n",
    "pbar = tqdm(total=tot_step, desc=\"Train\")\n",
    "for epoch in range(1000000):\n",
    "    print(f\"Begin epoch {epoch}\")\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        # if use_cuda:\n",
    "        #     inputs = inputs.cuda()\n",
    "        tot_train_time -= time.time()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        actual_step += 1\n",
    "\n",
    "        if actual_step % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
    "            glb_step += 1\n",
    "            if glb_step % pbar_update_freq == 0:\n",
    "                aveloss = (tot_loss - log_loss)/pbar_update_freq\n",
    "                pbar.update(10)\n",
    "                pbar.set_postfix({'loss': aveloss})\n",
    "                log_loss = tot_loss\n",
    "\n",
    "\n",
    "        # if optimizer1 is not None:\n",
    "        #     optimizer1.step()\n",
    "        #     optimizer1.zero_grad()\n",
    "        # if scheduler1 is not None:\n",
    "        #     scheduler1.step()\n",
    "        # if optimizer2 is not None:\n",
    "            # optimizer2.step()\n",
    "            # optimizer2.zero_grad()\n",
    "        # if scheduler2 is not None:\n",
    "            # scheduler2.step()\n",
    "\n",
    "\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        scheduler2.step()\n",
    "        \n",
    "        tot_train_time += time.time()\n",
    "\n",
    "        if actual_step % gradient_accumulation_steps == 0 and glb_step >0 and glb_step % 500 == 0:\n",
    "            val_acc = evaluate(prompt_model, validation_dataloader, desc=\"Valid\")\n",
    "            if val_acc >= best_val_acc:\n",
    "                torch.save(prompt_model.state_dict(),f\"/checkpoints/{this_run_unicode}.ckpt\")\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            acc_traces.append(val_acc)\n",
    "            print(\"Glb_step {}, val_acc {}, average time {}\".format(glb_step, val_acc, tot_train_time/actual_step ), flush=True)\n",
    "            prompt_model.train()\n",
    "\n",
    "        if glb_step > 2000:\n",
    "            leave_training = True\n",
    "            break\n",
    "\n",
    "    if leave_training:\n",
    "        break\n",
    "\n",
    "\n",
    "# # super_glue test split can not be evaluated without submitting the results to their website. So we skip it here and keep them as comments.\n",
    "#\n",
    "# prompt_model.load_state_dict(torch.load(f\"{args.project_root}/ckpts/{this_run_unicode}.ckpt\"))\n",
    "# prompt_model = prompt_model.cuda()\n",
    "# test_acc = evaluate(prompt_model, test_dataloader, desc=\"Test\")\n",
    "# test_acc = evaluate(prompt_model, test_dataloader, desc=\"Test\")\n",
    "\n",
    "# a simple measure for the convergence speed.\n",
    "thres99 = 0.99*best_val_acc\n",
    "thres98 = 0.98*best_val_acc\n",
    "thres100 = best_val_acc\n",
    "step100=step98=step99=20000\n",
    "for val_time, acc in enumerate(acc_traces):\n",
    "    if acc>=thres98:\n",
    "        step98 = min(val_time*500, step98)\n",
    "        if acc>=thres99:\n",
    "            step99 = min(val_time*500, step99)\n",
    "            if acc>=thres100:\n",
    "                step100 = min(val_time*500, step100)\n",
    "\n",
    "\n",
    "content_write += f\"BestValAcc:{best_val_acc}\\tEndValAcc:{acc_traces[-1]}\\tcritical_steps:{[step98,step99,step100]}\\n\"\n",
    "content_write += \"\\n\"\n",
    "\n",
    "print(content_write)\n",
    "\n",
    "with open(f\"/sfs_out/{this_run_unicode}_results.txt\", \"a\") as fout:\n",
    "    fout.write(content_write)\n",
    "\n",
    "# import os\n",
    "# os.remove(f\"../ckpts/{this_run_unicode}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd9df6-e119-4848-a527-9723ae42a701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
