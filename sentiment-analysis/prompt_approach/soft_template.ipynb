{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05de93-7b10-468e-9fca-b90a979626ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from openprompt.data_utils import PROCESSORS\n",
    "import torch\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import argparse\n",
    "import numpy as np\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.prompts import SoftTemplate\n",
    "from openprompt import PromptForClassification\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(\"\")\n",
    "# parser.add_argument(\"--shot\", type=int, default=-1)\n",
    "# parser.add_argument(\"--seed\", type=int, default=144)\n",
    "# parser.add_argument(\"--plm_eval_mode\", action=\"store_true\", help=\"whether to turn off the dropout in the freezed model. Set to true to turn off.\")\n",
    "# parser.add_argument(\"--tune_plm\", action=\"store_true\")\n",
    "# parser.add_argument(\"--model\", type=str, default='t5-lm', help=\"We test both t5 and t5-lm in this scripts, the corresponding tokenizerwrapper will be automatically loaded.\")\n",
    "# parser.add_argument(\"--model_name_or_path\", default='../../plm_cache/t5-large-lm-adapt/')\n",
    "# parser.add_argument(\"--project_root\", default=\"/mnt/sfs_turbo/hsd/OpenPrompt_official/OpenPrompt/\", help=\"The project root in the file system, i.e. the absolute path of OpenPrompt\")\n",
    "# parser.add_argument(\"--template_id\", type=int)\n",
    "# parser.add_argument(\"--verbalizer_id\", type=int)\n",
    "# parser.add_argument(\"--data_dir\", type=str, default=\"/mnt/sfs_turbo/huggingface_datasets/\") # sometimes, huggingface datasets can not be automatically downloaded due to network issue, please refer to 0_basic.py line 15 for solutions.\n",
    "# parser.add_argument(\"--dataset\",type=str)\n",
    "# parser.add_argument(\"--result_file\", type=str, default=\"../sfs_out/results.txt\")\n",
    "# parser.add_argument(\"--max_steps\", default=20000, type=int)\n",
    "# parser.add_argument(\"--prompt_lr\", type=float, default=0.3)\n",
    "# parser.add_argument(\"--warmup_step_prompt\", type=int, default=500)\n",
    "# parser.add_argument(\"--init_from_vocab\", action=\"store_false\")\n",
    "# parser.add_argument(\"--eval_every_steps\", type=int, default=500)\n",
    "# parser.add_argument(\"--soft_token_num\", type=int, default=20)\n",
    "# parser.add_argument(\"--optimizer\", type=str, default=\"Adafactor\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.result_file = os.path.join(args.project_root, args.result_file)\n",
    "\n",
    "# content_write = \"=\"*20+\"\\n\"\n",
    "# content_write += f\"dataset {args.dataset}\\t\"\n",
    "# content_write += f\"temp {args.template_id}\\t\"\n",
    "# content_write += f\"verb {args.verbalizer_id}\\t\"\n",
    "# content_write += f\"model {args.model}\\t\"\n",
    "# content_write += f\"seed {args.seed}\\t\"\n",
    "# content_write += f\"shot {args.shot}\\t\"\n",
    "# content_write += f\"plm_eval_mode {args.plm_eval_mode}\\t\"\n",
    "# content_write += f\"init_from_vocab {args.init_from_vocab}\\t\"\n",
    "# content_write += f\"eval_every_steps {args.eval_every_steps}\\t\"\n",
    "# content_write += f\"prompt_lr {args.prompt_lr}\\t\"\n",
    "# content_write += f\"optimizer {args.optimizer}\\t\"\n",
    "# content_write += f\"warmup_step_prompt {args.warmup_step_prompt}\\t\"\n",
    "# content_write += f\"soft_token_num {args.soft_token_num}\\t\"\n",
    "# content_write += \"\\n\"\n",
    "\n",
    "# print(content_write)\n",
    "\n",
    "import random\n",
    "this_run_unicode = str(random.randint(0, 1e10))\n",
    "\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# use lm-adapted version or t5-v1.1 checkpoint. Note that the original t5 checkpoint has been pretrained\n",
    "# on part of GLUE dataset, thus should not be used.\n",
    "# from openprompt.plms.seq2seq import T5TokenizerWrapper, T5LMTokenizerWrapper\n",
    "from mlm import MLMTokenizerWrapper\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, set_seed\n",
    "# from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "from openprompt.data_utils.data_sampler import FewShotSampler\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "MODEL = 'UFNLP/gatortron-base'\n",
    "model_path = \"../runs/ta_pretraining/checkpoint-435\"\n",
    "model_config = BertConfig.from_pretrained(model_path)\n",
    "plm = BertForMaskedLM.from_pretrained(model_path, config=model_config)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "WrapperClass = MLMTokenizerWrapper\n",
    "#plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e166c-e0af-4e7b-8c50-fb25fcf2b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = {}\n",
    "\n",
    "# Below are multiple dataset examples, including few-shot ones.\n",
    "# if args.dataset == \"boolq\":\n",
    "#     Processor = PROCESSORS[\"super_glue.boolq\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/BoolQ\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480 # this should be specified according to the running GPU's capacity\n",
    "#     if args.tune_plm: # tune the entire plm will use more gpu-memories, thus we should use a smaller batch_size.\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True # if multiple gpus are available, one can use model_parallelize\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"multirc\":\n",
    "#     Processor = PROCESSORS[\"super_glue.multirc\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/MultiRC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"rte\":\n",
    "#     Processor = PROCESSORS[\"super_glue.rte\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/RTE\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 2\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"cb\":\n",
    "#     Processor = PROCESSORS[\"super_glue.cb\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/CB\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"wic\":\n",
    "#     Processor = PROCESSORS[\"super_glue.wic\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/WiC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"fewshot_boolq\":\n",
    "#     Processor = PROCESSORS[\"super_glue.boolq\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/BoolQ\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     sampler = FewShotSampler(num_examples_per_label=32)\n",
    "#     dataset['train']= sampler(dataset['train'], seed=args.seed)\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"fewshot_multirc\":\n",
    "#     Processor = PROCESSORS[\"super_glue.multirc\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/MultiRC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     sampler = FewShotSampler(num_examples_per_label=32)\n",
    "#     dataset['train']= sampler(dataset['train'], seed=args.seed)\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# elif args.dataset == \"fewshot_wic\":\n",
    "#     Processor = PROCESSORS[\"super_glue.wic\"]\n",
    "#     dataset['train'] = Processor().get_train_examples(args.data_dir)\n",
    "#     dataset['validation'] = Processor().get_dev_examples(args.data_dir)\n",
    "#     dataset['test'] = Processor().get_test_examples(args.data_dir)\n",
    "#     class_labels =Processor().get_labels()\n",
    "#     scriptsbase = \"SuperGLUE/WiC\"\n",
    "#     scriptformat = \"txt\"\n",
    "#     sampler = FewShotSampler(num_examples_per_label=32)\n",
    "#     dataset['train']= sampler(dataset['train'], seed=args.seed)\n",
    "#     max_seq_l = 480\n",
    "#     if args.tune_plm:\n",
    "#         batchsize_t = 4\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 8\n",
    "#         model_parallelize = True\n",
    "#     else:\n",
    "#         batchsize_t = 8\n",
    "#         batchsize_e = 4\n",
    "#         gradient_accumulation_steps = 4\n",
    "#         model_parallelize = False\n",
    "# else:\n",
    "#     raise NotImplementedError\n",
    "\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "from openprompt.data_utils import InputExample\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_labels(sentiment):\n",
    "    labels = []\n",
    "    for s in sentiment:\n",
    "        if s == 'neutral':\n",
    "            labels += [0]\n",
    "        elif s == 'negative':\n",
    "            labels += [1]\n",
    "        else:\n",
    "            labels += [2]\n",
    "    return labels\n",
    "\n",
    "# Create task Dataset from annotated samples\n",
    "sentences = pd.read_csv('../data/sentences_MD_wContext.csv', header=0)\n",
    "sentences = sentences[['idx','language', \"MD_label\", \"context\"]]\n",
    "\n",
    "dataset = Dataset.from_pandas(sentences).rename_columns({'language': 'sentence', \"MD_label\": 'sentiment'})\n",
    "dataset = dataset.add_column('label', create_labels(dataset['sentiment']))\n",
    "train_test = dataset.train_test_split(0.35, seed = seed)\n",
    "dev_test = train_test['test'].train_test_split(0.5, seed = seed)\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': dev_test['train'],\n",
    "    'test': dev_test['test']})\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        input_example = InputExample(text_a = data['sentence'], text_b = data['context'], \n",
    "                                     label=int(data['label']), guid=data['idx'])\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])\n",
    "\n",
    "\n",
    "# class_labels =Processor().get_labels() # what do i need to update this to? best bet below\n",
    "class_labels = [ # There are two classes in Sentiment Analysis, one for negative and one for positive\n",
    "    \"neutral\",\n",
    "    \"negative\",\n",
    "    \"positive\"\n",
    "]\n",
    "sampler = FewShotSampler(num_examples_per_label=32)\n",
    "dataset['train']= sampler(dataset['train'], seed=seed)\n",
    "max_seq_l = 480\n",
    "batchsize_t = 4\n",
    "batchsize_e = 4\n",
    "gradient_accumulation_steps = 8\n",
    "model_parallelize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be975b-19ef-4fca-b465-659fdde0a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the template and verbalizer.\n",
    "# Note that soft template can be combined with hard template, by loading the hard template from file.\n",
    "# For example, the template in soft_template.txt is {}\n",
    "# The choice_id 1 is the hard template\n",
    "mytemplate = SoftTemplate(model=plm, \n",
    "                          tokenizer=tokenizer, \n",
    "                          num_tokens=args.soft_token_num, \n",
    "                          initialize_from_vocab=args.init_from_vocab).from_file(f\"scripts/{scriptsbase}/soft_template.txt\", \n",
    "                                                                                choice=args.template_id)\n",
    "myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\", choice=args.verbalizer_id)\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)\n",
    "\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=(not args.tune_plm), plm_eval_mode=args.plm_eval_mode)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "if model_parallelize:\n",
    "    prompt_model.parallelize()\n",
    "\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "    batch_size=batchsize_t,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "    batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "# zero-shot test\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "    batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "print(\"truncate rate: {}\".format(test_dataloader.tokenizer_wrapper.truncate_rate), flush=True)\n",
    "\n",
    "def evaluate(prompt_model, dataloader, desc):\n",
    "    prompt_model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup,get_constant_schedule_with_warmup  # use AdamW is a standard practice for transformer\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule  # use Adafactor is the default setting for T5\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "tot_step = args.max_steps\n",
    "\n",
    "\n",
    "if args.tune_plm: # normally we freeze the model when using soft_template. However, we keep the option to tune plm\n",
    "    no_decay = ['bias', 'LayerNorm.weight'] # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "    optimizer_grouped_parameters1 = [\n",
    "        {'params': [p for n, p in prompt_model.plm.named_parameters() if (not any(nd in n for nd in no_decay))], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
    "    scheduler1 = get_linear_schedule_with_warmup(\n",
    "        optimizer1,\n",
    "        num_warmup_steps=500, num_training_steps=tot_step)\n",
    "else:\n",
    "    optimizer1 = None\n",
    "    scheduler1 = None\n",
    "\n",
    "\n",
    "optimizer_grouped_parameters2 = [{'params': [p for name, p in prompt_model.template.named_parameters() if 'raw_embedding' not in name]}] # note that you have to remove the raw_embedding manually from the optimization\n",
    "if args.optimizer.lower() == \"adafactor\":\n",
    "    optimizer2 = Adafactor(optimizer_grouped_parameters2,\n",
    "                            lr=args.prompt_lr,\n",
    "                            relative_step=False,\n",
    "                            scale_parameter=False,\n",
    "                            warmup_init=False)  # when lr is 0.3, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
    "    scheduler2 = get_constant_schedule_with_warmup(optimizer2, num_warmup_steps=args.warmup_step_prompt) # when num_warmup_steps is 0, it is the same as the configuration of https://arxiv.org/abs/2104.08691\n",
    "elif args.optimizer.lower() == \"adamw\":\n",
    "    optimizer2 = AdamW(optimizer_grouped_parameters2, lr=args.prompt_lr) # usually lr = 0.5\n",
    "    scheduler2 = get_linear_schedule_with_warmup(\n",
    "                    optimizer2,\n",
    "                    num_warmup_steps=args.warmup_step_prompt, num_training_steps=tot_step) # usually num_warmup_steps is 500\n",
    "\n",
    "\n",
    "tot_loss = 0\n",
    "log_loss = 0\n",
    "best_val_acc = 0\n",
    "glb_step = 0\n",
    "actual_step = 0\n",
    "leave_training = False\n",
    "\n",
    "acc_traces = []\n",
    "tot_train_time = 0\n",
    "pbar_update_freq = 10\n",
    "prompt_model.train()\n",
    "\n",
    "pbar = tqdm(total=tot_step, desc=\"Train\")\n",
    "for epoch in range(1000000):\n",
    "    print(f\"Begin epoch {epoch}\")\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        tot_train_time -= time.time()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        actual_step += 1\n",
    "\n",
    "        if actual_step % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
    "            glb_step += 1\n",
    "            if glb_step % pbar_update_freq == 0:\n",
    "                aveloss = (tot_loss - log_loss)/pbar_update_freq\n",
    "                pbar.update(10)\n",
    "                pbar.set_postfix({'loss': aveloss})\n",
    "                log_loss = tot_loss\n",
    "\n",
    "\n",
    "        if optimizer1 is not None:\n",
    "            optimizer1.step()\n",
    "            optimizer1.zero_grad()\n",
    "        if scheduler1 is not None:\n",
    "            scheduler1.step()\n",
    "        if optimizer2 is not None:\n",
    "            optimizer2.step()\n",
    "            optimizer2.zero_grad()\n",
    "        if scheduler2 is not None:\n",
    "            scheduler2.step()\n",
    "\n",
    "        tot_train_time += time.time()\n",
    "\n",
    "        if actual_step % gradient_accumulation_steps == 0 and glb_step >0 and glb_step % args.eval_every_steps == 0:\n",
    "            val_acc = evaluate(prompt_model, validation_dataloader, desc=\"Valid\")\n",
    "            if val_acc >= best_val_acc:\n",
    "                torch.save(prompt_model.state_dict(),f\"{args.project_root}/../ckpts/{this_run_unicode}.ckpt\")\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            acc_traces.append(val_acc)\n",
    "            print(\"Glb_step {}, val_acc {}, average time {}\".format(glb_step, val_acc, tot_train_time/actual_step ), flush=True)\n",
    "            prompt_model.train()\n",
    "\n",
    "        if glb_step > args.max_steps:\n",
    "            leave_training = True\n",
    "            break\n",
    "\n",
    "    if leave_training:\n",
    "        break\n",
    "\n",
    "\n",
    "# # super_glue test split can not be evaluated without submitting the results to their website. So we skip it here and keep them as comments.\n",
    "#\n",
    "# prompt_model.load_state_dict(torch.load(f\"{args.project_root}/ckpts/{this_run_unicode}.ckpt\"))\n",
    "# prompt_model = prompt_model.cuda()\n",
    "# test_acc = evaluate(prompt_model, test_dataloader, desc=\"Test\")\n",
    "# test_acc = evaluate(prompt_model, test_dataloader, desc=\"Test\")\n",
    "\n",
    "# a simple measure for the convergence speed.\n",
    "thres99 = 0.99*best_val_acc\n",
    "thres98 = 0.98*best_val_acc\n",
    "thres100 = best_val_acc\n",
    "step100=step98=step99=args.max_steps\n",
    "for val_time, acc in enumerate(acc_traces):\n",
    "    if acc>=thres98:\n",
    "        step98 = min(val_time*args.eval_every_steps, step98)\n",
    "        if acc>=thres99:\n",
    "            step99 = min(val_time*args.eval_every_steps, step99)\n",
    "            if acc>=thres100:\n",
    "                step100 = min(val_time*args.eval_every_steps, step100)\n",
    "\n",
    "\n",
    "content_write += f\"BestValAcc:{best_val_acc}\\tEndValAcc:{acc_traces[-1]}\\tcritical_steps:{[step98,step99,step100]}\\n\"\n",
    "content_write += \"\\n\"\n",
    "\n",
    "print(content_write)\n",
    "\n",
    "with open(f\"{args.result_file}\", \"a\") as fout:\n",
    "    fout.write(content_write)\n",
    "\n",
    "import os\n",
    "os.remove(f\"../ckpts/{this_run_unicode}.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
