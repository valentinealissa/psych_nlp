{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df98387-a695-47ff-9bc6-6bceff5b8738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'class'],\n",
      "        num_rows: 23\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'class'],\n",
      "        num_rows: 24\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
      "        num_rows: 48\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_metric\n",
    "import random\n",
    "from transformers import BartTokenizerFast, BartForSequenceClassification, Trainer, TrainingArguments, EvalPrediction, pipeline, set_seed, DataCollatorWithPadding\n",
    "\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)\n",
    "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large-mnli')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "df = pd.read_csv('../data/MD-NLI.csv', header=0)\n",
    "df = df[[\"language\", \"MD_label\"]]\n",
    "dataset = Dataset.from_pandas(df).rename_columns({'language': 'text', \"MD_label\": 'class'})\n",
    "label_dt = dataset.train_test_split(0.5, seed = seed)\n",
    "# label_dt = DatasetDict({\n",
    "#     'train': train_test['train'],\n",
    "#     'dev': dev_test['train'],\n",
    "#     'test': dev_test['test']})\n",
    "\n",
    "print(label_dt)\n",
    "\n",
    "label_to_int = [\"neutral\", \"negative\", \"positive\"]\n",
    "# label_to_int = [0, 1, 2]\n",
    "template = \"The sentiment of this sentence is {}\"\n",
    "\n",
    "def create_input_sequence(sample):\n",
    "    text = sample[\"text\"]\n",
    "    label = sample[\"class\"][0]\n",
    "    contradiction_label = random.choice([x for x in label_to_int if x != label])\n",
    "    encoded_sequence = tokenizer(text * 2, [template.format(label), template.format(contradiction_label)], truncation = True, padding = 'max_length')\n",
    "    encoded_sequence[\"labels\"] = [2, 0]\n",
    "    encoded_sequence[\"input_sentence\"] = tokenizer.batch_decode(encoded_sequence.input_ids)\n",
    "    return encoded_sequence\n",
    "\n",
    "\n",
    "# # Split to train and test portions\n",
    "# df_train = df.head(train_portion)\n",
    "# df_test = df.tail(test_portion)\n",
    "# # Convert to Dataset objects\n",
    "# train_ds = Dataset.from_pandas(df_train, split=\"train\")\n",
    "# test_ds = Dataset.from_pandas(df_test, split=\"test\")\n",
    "# # create mappings\n",
    "label_dt = label_dt.map(create_input_sequence, batched = True, batch_size = 1, remove_columns = [\"class\", \"text\"])\n",
    "# test_dataset = test_ds.map(create_input_sequence, batched = True, batch_size = 1, remove_columns = [\"class\", \"text\"])\n",
    "print(label_dt)\n",
    "model = BartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\", num_labels = len(label_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a92011-770b-4a8b-97f9-f3c639eaba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valena17/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/Users/valena17/miniconda3/envs/torch-gpu/lib/python3.8/multiprocessing/resource_tracker.py:96: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n",
      "Fatal Python error: config_get_locale_encoding: failed to get the locale encoding: nl_langinfo(CODESET) failed\n",
      "Python runtime state: preinitialized\n",
      "\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Fatal Python error: config_get_locale_encoding: failed to get the locale encoding: nl_langinfo(CODESET) failed\n",
      "Python runtime state: preinitialized\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 09:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fatal Python error: config_get_locale_encoding: failed to get the locale encoding: nl_langinfo(CODESET) failed\n",
      "Python runtime state: preinitialized\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 02:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/m4dy5bvs1fn7fb9wxjhsg4dc0000gn/T/ipykernel_25275/184399543.py:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric_acc = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valena17/miniconda3/envs/torch-gpu/lib/python3.8/multiprocessing/resource_tracker.py:96: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6754849553108215,\n",
       " 'eval_accuracy': 0.5208333333333334,\n",
       " 'eval_f1': 0.48290398126463696,\n",
       " 'eval_precision': 0.5294840294840294,\n",
       " 'eval_recall': 0.5208333333333334,\n",
       " 'eval_runtime': 173.1239,\n",
       " 'eval_samples_per_second': 0.277,\n",
       " 'eval_steps_per_second': 0.035,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fatal Python error: config_get_locale_encoding: failed to get the locale encoding: nl_langinfo(CODESET) failed\n",
      "Python runtime state: preinitialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'runs/MD',      # Output directory\n",
    "    num_train_epochs = 1,             # Total number of training epochs\n",
    "    # per_device_train_batch_size = 16,  # Batch size per device during training\n",
    "    # per_device_eval_batch_size = 64,   # Batch size for evaluation\n",
    "    warmup_ratio = 0.01,                # Warmup ratio for learning rate scheduler\n",
    "    weight_decay = 0.01,               # Strength of weight decay\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    metric_precision = load_metric(\"precision\")\n",
    "    metric_recall = load_metric(\"recall\")\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    result = {}\n",
    "    result[\"accuracy\"] = metric_acc.compute(predictions = preds, references = p.label_ids)[\"accuracy\"]\n",
    "    result[\"f1\"] = metric_f1.compute(predictions = preds, references = p.label_ids, pos_label=1, average = \"weighted\")[\"f1\"] #play with \"weighted\" \"micro\" \"macro\"\n",
    "    result[\"precision\"] = metric_precision.compute(predictions = preds, references = p.label_ids, pos_label=1, average=\"weighted\", sample_weight=None, zero_division='warn')[\"precision\"]\n",
    "    result[\"recall\"] = metric_recall.compute(predictions = preds, references = p.label_ids, pos_label=1, average=\"weighted\", sample_weight=None, zero_division='warn')[\"recall\"]\n",
    "    return result\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,                     # The instantiated model to be trained\n",
    "    args = training_args,              # Training arguments, defined above\n",
    "    compute_metrics = compute_metrics, # A function to compute the metrics\n",
    "    train_dataset = label_dt['train'],     # Training dataset\n",
    "    eval_dataset = label_dt['test'],       # Evaluation dataset\n",
    "    tokenizer = tokenizer,              # The tokenizer that was used\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "# model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aad36d-5ff9-4496-94b0-2584878552f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model = model, tokenizer = tokenizer, device = 0)\n",
    "classifier(sequences, label_to_int, multi_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
